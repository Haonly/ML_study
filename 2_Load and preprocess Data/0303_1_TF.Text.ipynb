{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TF.Text\n",
    "- text와 관련된 class 제공\n",
    "- text-based model에서 필요한 preprocessing을 처리할 수 있도록 하는 library 제공\n",
    "\n",
    "# 2. Eager Execution(즉시 실행)\n",
    "- 그래프를 생성하지 않고 함수를 바로 실행하는 명령형 프로그래밍 환경으로 나중에 실행하기 위해 계산가능한 그래프를 생성하는 대신에 계산값을 즉시 알려주는 연산\n",
    "- 연구와 실험을 위한 플랫폼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unicode\n",
    "- UTF-8은 Unicode를 인코딩 하는 방식임\n",
    "- String을 UTF-8 방식으로 바꿔주거나 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
    "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenization\n",
    "- string 을 tokens으로 나눠주는 것\n",
    "- tokens는 단어(words), 숫자(numbers), 또는 punctuation을 말함\n",
    "\n",
    "<b>`Tokenizer`</b>와 <b>`TokenizerWithOffsets`</b>은 각각 <b>`tokenize`</b>와 <b>`tokenize_with_offsets`</b> 메서드를 가짐\n",
    "\n",
    "<b>`TokenizerWithOffsets`</b>은 byte offset을 original string으로 변환함\n",
    "\n",
    "<b>`Tokenizer`</b>들은 <i>`RagggedTensors(비정형 텐서)`</i>를 반환함\n",
    "  - 비정형 텐서는 가변길이의 데이터를 쉽게 저장하고 처리할 수 있음\n",
    "  - 수학 연산 (예 : `tf.add` 및 `tf.reduce_mean`), 배열 연산 (예 : `tf.concat` 및 `tf.tile`), 문자열 조작 작업 (예 : `tf.substr`)을 포함하여 수백 가지 이상의 텐서플로 연산에서 지원\n",
    "  - [비정형텐서 참고](https://www.tensorflow.org/guide/ragged_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer\n",
    "- UTF-8 string의 whitespace를 나눠줌(스페이스, 탭, 줄바꿈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnicodeScriptTokenizer\n",
    "- Unicode stript의 바운더리에 기반하여 UTF-8 string을 나눠줌(split)\n",
    "- WhitespaceTokenizer와 거의 비슷한 역할을 하지만 추가로 구두점(punctuation, USCRIPT_COMMON)을 language text(USCRIPT_LATIN, USCRIPT_CYRILLIC 등)로 바꿔줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xf0\\x9f\\x99\\x81']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad🙁'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode split\n",
    "- 언어(문장)을 whitespace 없이 단어로 토크나이징할 때 <i>unicode_split</i>을 사용해 철자(character)로 나눔(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offsets\n",
    "- strings(문장)을 토크나이즈 할 때 original string이 어디서 왔는지 알아야 함\n",
    "- 그래서 `TokenizerWithOffsets`을 implement하는 모든 `tokenizer`는 토큰과 함께 byte offset을 함께 리턴하는 `tokenize_with_offsets` 메서드를 가짐\n",
    "- `offset_starts`는 각 token이 시작하는 original string의 byte를 리스트 함\n",
    "- `offset_limits`는 각 토큰이 끝나는 byte를 리스트 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
      "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
      "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())\n",
    "print(offset_starts.to_list())\n",
    "print(offset_limits.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.Data Example\n",
    "- tokenizer는 tf.data.API 가 기대하는데로 작동함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x13a009170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: cannot import name 'DenseNet121' from 'tensorflow.python.keras.applications' (/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/__init__.py)\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x13a009170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: cannot import name 'DenseNet121' from 'tensorflow.python.keras.applications' (/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/__init__.py)\n",
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Text ops\n",
    "- `tf.text` 는 다른 전처리 기능을 포함함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Wordshape\n",
    "- NLP model에서 고려하는 feature는 text string이 특정한 특징을 가지고 있는지 분석하는 것임.\n",
    "- 예를 들어 이모티콘이 있거나 대문자로 쓰여진 단어를 확인함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False, False], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n",
      "[[False, False, False, False, False, True], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Is capitalized?\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
    "# Are all letters uppercased?\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
    "# Does the token contain punctuation?\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
    "# Is the token a number?\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
    "\n",
    "print(f1.to_list())\n",
    "print(f2.to_list())\n",
    "print(f3.to_list())\n",
    "print(f4.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) N-grams & Sliding Window\n",
    "- 단어의 연속 with <i>n</i> 크기를 가지는 sliding window\n",
    "- `Reduction.STRING_JOIN`을 통해 string을 각각에 붙여 tokens를 축소를 할 수 있음\n",
    "- 디폴트는 띄어쓰기이며 `string_separater`로 조정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Ngrams, in this case bi-gram (n = 2)\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
