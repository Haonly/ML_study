{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TF.Text\n",
    "- textì™€ ê´€ë ¨ëœ class ì œê³µ\n",
    "- text-based modelì—ì„œ í•„ìš”í•œ preprocessingì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” library ì œê³µ\n",
    "\n",
    "# 2. Eager Execution(ì¦‰ì‹œ ì‹¤í–‰)\n",
    "- ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì§€ ì•Šê³  í•¨ìˆ˜ë¥¼ ë°”ë¡œ ì‹¤í–‰í•˜ëŠ” ëª…ë ¹í˜• í”„ë¡œê·¸ë˜ë° í™˜ê²½ìœ¼ë¡œ ë‚˜ì¤‘ì— ì‹¤í–‰í•˜ê¸° ìœ„í•´ ê³„ì‚°ê°€ëŠ¥í•œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹ ì— ê³„ì‚°ê°’ì„ ì¦‰ì‹œ ì•Œë ¤ì£¼ëŠ” ì—°ì‚°\n",
    "- ì—°êµ¬ì™€ ì‹¤í—˜ì„ ìœ„í•œ í”Œë«í¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unicode\n",
    "- UTF-8ì€ Unicodeë¥¼ ì¸ì½”ë”© í•˜ëŠ” ë°©ì‹ì„\n",
    "- Stringì„ UTF-8 ë°©ì‹ìœ¼ë¡œ ë°”ê¿”ì£¼ê±°ë‚˜ ë˜ì–´ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sadâ˜¹'.encode('UTF-16-BE')])\n",
    "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenization\n",
    "- string ì„ tokensìœ¼ë¡œ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ\n",
    "- tokensëŠ” ë‹¨ì–´(words), ìˆ«ì(numbers), ë˜ëŠ” punctuationì„ ë§í•¨\n",
    "\n",
    "<b>`Tokenizer`</b>ì™€ <b>`TokenizerWithOffsets`</b>ì€ ê°ê° <b>`tokenize`</b>ì™€ <b>`tokenize_with_offsets`</b> ë©”ì„œë“œë¥¼ ê°€ì§\n",
    "\n",
    "<b>`TokenizerWithOffsets`</b>ì€ byte offsetì„ original stringìœ¼ë¡œ ë³€í™˜í•¨\n",
    "\n",
    "<b>`Tokenizer`</b>ë“¤ì€ <i>`RagggedTensors(ë¹„ì •í˜• í…ì„œ)`</i>ë¥¼ ë°˜í™˜í•¨\n",
    "  - ë¹„ì •í˜• í…ì„œëŠ” ê°€ë³€ê¸¸ì´ì˜ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì €ì¥í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ\n",
    "  - ìˆ˜í•™ ì—°ì‚° (ì˜ˆ : `tf.add` ë° `tf.reduce_mean`), ë°°ì—´ ì—°ì‚° (ì˜ˆ : `tf.concat` ë° `tf.tile`), ë¬¸ìì—´ ì¡°ì‘ ì‘ì—… (ì˜ˆ : `tf.substr`)ì„ í¬í•¨í•˜ì—¬ ìˆ˜ë°± ê°€ì§€ ì´ìƒì˜ í…ì„œí”Œë¡œ ì—°ì‚°ì—ì„œ ì§€ì›\n",
    "  - [ë¹„ì •í˜•í…ì„œ ì°¸ê³ ](https://www.tensorflow.org/guide/ragged_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer\n",
    "- UTF-8 stringì˜ whitespaceë¥¼ ë‚˜ëˆ ì¤Œ(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sadâ˜¹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnicodeScriptTokenizer\n",
    "- Unicode striptì˜ ë°”ìš´ë”ë¦¬ì— ê¸°ë°˜í•˜ì—¬ UTF-8 stringì„ ë‚˜ëˆ ì¤Œ(split)\n",
    "- WhitespaceTokenizerì™€ ê±°ì˜ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ì§€ë§Œ ì¶”ê°€ë¡œ êµ¬ë‘ì (punctuation, USCRIPT_COMMON)ì„ language text(USCRIPT_LATIN, USCRIPT_CYRILLIC ë“±)ë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xf0\\x9f\\x99\\x81']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'SadğŸ™'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode split\n",
    "- ì–¸ì–´(ë¬¸ì¥)ì„ whitespace ì—†ì´ ë‹¨ì–´ë¡œ í† í¬ë‚˜ì´ì§•í•  ë•Œ <i>unicode_split</i>ì„ ì‚¬ìš©í•´ ì² ì(character)ë¡œ ë‚˜ëˆ”(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tf.strings.unicode_split([u\"ä»…ä»Šå¹´å‰\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offsets\n",
    "- strings(ë¬¸ì¥)ì„ í† í¬ë‚˜ì´ì¦ˆ í•  ë•Œ original stringì´ ì–´ë””ì„œ ì™”ëŠ”ì§€ ì•Œì•„ì•¼ í•¨\n",
    "- ê·¸ë˜ì„œ `TokenizerWithOffsets`ì„ implementí•˜ëŠ” ëª¨ë“  `tokenizer`ëŠ” í† í°ê³¼ í•¨ê»˜ byte offsetì„ í•¨ê»˜ ë¦¬í„´í•˜ëŠ” `tokenize_with_offsets` ë©”ì„œë“œë¥¼ ê°€ì§\n",
    "- `offset_starts`ëŠ” ê° tokenì´ ì‹œì‘í•˜ëŠ” original stringì˜ byteë¥¼ ë¦¬ìŠ¤íŠ¸ í•¨\n",
    "- `offset_limits`ëŠ” ê° í† í°ì´ ëë‚˜ëŠ” byteë¥¼ ë¦¬ìŠ¤íŠ¸ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
      "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
      "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sadâ˜¹'.encode('UTF-8')])\n",
    "print(tokens.to_list())\n",
    "print(offset_starts.to_list())\n",
    "print(offset_limits.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.Data Example\n",
    "- tokenizerëŠ” tf.data.API ê°€ ê¸°ëŒ€í•˜ëŠ”ë°ë¡œ ì‘ë™í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x13a009170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: cannot import name 'DenseNet121' from 'tensorflow.python.keras.applications' (/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/__init__.py)\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x13a009170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: cannot import name 'DenseNet121' from 'tensorflow.python.keras.applications' (/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/__init__.py)\n",
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Text ops\n",
    "- `tf.text` ëŠ” ë‹¤ë¥¸ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Wordshape\n",
    "- NLP modelì—ì„œ ê³ ë ¤í•˜ëŠ” featureëŠ” text stringì´ íŠ¹ì •í•œ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ë¶„ì„í•˜ëŠ” ê²ƒì„.\n",
    "- ì˜ˆë¥¼ ë“¤ì–´ ì´ëª¨í‹°ì½˜ì´ ìˆê±°ë‚˜ ëŒ€ë¬¸ìë¡œ ì“°ì—¬ì§„ ë‹¨ì–´ë¥¼ í™•ì¸í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False, False], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n",
      "[[False, False, False, False, False, True], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sadâ˜¹'.encode('UTF-8')])\n",
    "\n",
    "# Is capitalized?\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
    "# Are all letters uppercased?\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
    "# Does the token contain punctuation?\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
    "# Is the token a number?\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
    "\n",
    "print(f1.to_list())\n",
    "print(f2.to_list())\n",
    "print(f3.to_list())\n",
    "print(f4.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) N-grams & Sliding Window\n",
    "- ë‹¨ì–´ì˜ ì—°ì† with <i>n</i> í¬ê¸°ë¥¼ ê°€ì§€ëŠ” sliding window\n",
    "- `Reduction.STRING_JOIN`ì„ í†µí•´ stringì„ ê°ê°ì— ë¶™ì—¬ tokensë¥¼ ì¶•ì†Œë¥¼ í•  ìˆ˜ ìˆìŒ\n",
    "- ë””í´íŠ¸ëŠ” ë„ì–´ì“°ê¸°ì´ë©° `string_separater`ë¡œ ì¡°ì • ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sadâ˜¹'.encode('UTF-8')])\n",
    "\n",
    "# Ngrams, in this case bi-gram (n = 2)\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
